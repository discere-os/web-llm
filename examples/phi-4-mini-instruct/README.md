# WebLLM Phi-4-mini-instruct Example

This example demonstrates how to use the `mlc-ai/Phi-4-mini-instruct-q4f16_1-MLC` model with the WebLLM engine, while also showcasing how to retrieve and display performance metrics for each inference.

## How to Run

1.  **Install Dependencies:**
    ```bash
    npm install
    ```

2.  **Start the Application:**
    ```bash
    npm start
    ```

This will start a local development server. Open your browser and navigate to the provided URL (usually `http://localhost:8888`) to see the output in the developer console.
