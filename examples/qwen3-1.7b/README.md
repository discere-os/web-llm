# WebLLM Qwen3-1.7B Example

This example demonstrates how to use the `Qwen3-1.7B-q4f16_1-MLC` model with the WebLLM engine, while also showcasing how to retrieve and display performance metrics for each inference.

## How to Run

1.  **Install Dependencies:**
    ```bash
    npm install
    ```

2.  **Start the Application:**
    ```bash
    npm start
    ```

This will start a local development server. Open your browser and navigate to the provided URL (usually `http://localhost:8888`) to see the output in the developer console.
