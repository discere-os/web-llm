# WebLLM Gemma-3-1B-IT Example

This example demonstrates how to use the `mlc-ai/gemma-3-1b-it-q4f16_1-MLC` model with the WebLLM engine, while also showcasing how to retrieve and display performance metrics for each inference.

## How to Run

1.  **Install Dependencies:**
    ```bash
    npm install
    ```

2.  **Start the Application:**
    ```bash
    npm start
    ```

This will start a local development server. Open your browser and navigate to the provided URL (usually `http://localhost:8888`) to see the output in the developer console.
